# 书生浦语大模型第一课

## 1 大模型相关背景

![image-20240105165438163](InternLM-learning\img\img_1\image-20240105165438163.png)



![image-20240105172534261](D:\项目文件\课程学习\InternLM学习\img\img_1\image-20240105172534261.png)

大模型成为从专用模型到通用模型的重要途径

## 2 书生浦语大模型

### 2.1 InternLM简介

- 轻量级：InternLM-7B
- 中量级：InternLM-20B
- 重量级：InternLM-123B

![image-20240105192846280](D:\项目文件\课程学习\InternLM学习\img\img_1\image-20240105192846280.png)



从模型到应用

大模型应用比如智能客服、个人助手、行业应用

![image-20240105193002124](D:\项目文件\课程学习\InternLM学习\img\img_1\image-20240105193002124.png)

### 2.2 InternLM开源生态

![image-20240105193312722](D:\项目文件\课程学习\InternLM学习\img\img_1\image-20240105193312722.png)

 数据： 书生万卷

 预训练：InternLM-Train

 微调： XTuner(支持全量/LoRA等微调)

 部署： LMDeploy

 评测： OpenCompass

 应用： Lagent AgentLego

![image-20240105193948315](D:\项目文件\课程学习\InternLM学习\img\img_1\image-20240105193948315.png)

 微调

**增量续训**：让基座模型学习新知识，垂直领域

**有监督微调**：让模型学会理解和遵循各种指令。

一般采用全量参数微调和部分参数微调等方法。

**多种微调算法**：多种微调策略与算法，覆盖各类SFT场景。

**适配多种开源生态**：支持加载HuggingFace、ModelScope模型或者数据级

**自动优化加速**：开发者无需关注复杂的显存优化和计算加速细节![image-20240105194822723](D:\项目文件\课程学习\InternLM学习\img\img_1\image-20240105194822723.png)

![image-20240105195757919](D:\项目文件\课程学习\InternLM学习\img\img_1\image-20240105195757919.png)

![image-20240105195039569](D:\项目文件\课程学习\InternLM学习\img\img_1\image-20240105195039569.png)

**多模态智能体工具箱AgentLego**

- 丰富的工具集合，尤其是提供了大量视觉、多模态相关领域的
- 支持多个主流智能体系统，如LangChain。Transformers Agent， Lagent等。
- 灵活的多模态工具调用接口，可以轻松支持各类输入输出格式的工具函数
- 一键式远程工具部署，轻松使用和调试大模型智能体。

